var documenterSearchIndex = {"docs":
[{"location":"gettingstarted/#Computing-Approximate-Likelihoods-with-Probabilistic-Numerics-and-Fenrir.jl","page":"Fenrr.jl in a nutshell","title":"Computing Approximate Likelihoods with Probabilistic Numerics and Fenrir.jl","text":"","category":"section"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"using LinearAlgebra\nusing OrdinaryDiffEq, ProbNumDiffEq, Plots\nusing Fenrir\nusing Optimization, OptimizationOptimJL\nstack(x) = copy(reduce(hcat, x)') # convenient\nnothing # hide","category":"page"},{"location":"gettingstarted/#The-parameter-inference-problem-in-general","page":"Fenrr.jl in a nutshell","title":"The parameter inference problem in general","text":"","category":"section"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"Let's assume we have an initial value problem (IVP)","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"beginaligned\ndoty = f_theta(y t) qquad y(t_0) = y_0\nendaligned","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"which we observe through a set mathcalD = u(t_n)_n=1^N of noisy data points","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"beginaligned\nu(t_n) = H y(t_n) + v_n qquad v_n sim mathcalN(0 R)\nendaligned","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"The question of interest is: How can we compute the marginal likelihood p(mathcalD mid theta)? Short answer: We can't. It's intractable, because exactly computing the true IVP solution y(t) is intractable. What we can do however is compute an approximate marginal likelihood. This is what Fenrir.jl provides. For details, check out the paper.","category":"page"},{"location":"gettingstarted/#The-specific-problem,-in-code","page":"Fenrr.jl in a nutshell","title":"The specific problem, in code","text":"","category":"section"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"Let's assume that the true underlying dynamics are given by a FitzHugh-Nagumo model","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"function f(du, u, p, t)\n    a, b, c = p\n    du[1] = c*(u[1] - u[1]^3/3 + u[2])\n    du[2] = -(1/c)*(u[1] -  a - b*u[2])\nend\nu0 = [-1.0, 1.0]\ntspan = (0.0, 20.0)\np = (0.2, 0.2, 3.0)\ntrue_prob = ODEProblem(f, u0, tspan, p)","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"from which we generate some artificial noisy data","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"true_sol = solve(true_prob, Vern9(), abstol=1e-10, reltol=1e-10)\n\ntimes = 1:0.5:20\nobservation_noise_var = 1e-1\nodedata = [true_sol(t) .+ sqrt(observation_noise_var) * randn(length(u0)) for t in times]\n\nplot(true_sol, color=:black, linestyle=:dash, label=[\"True Solution\" \"\"])\nscatter!(times, stack(odedata), markersize=2, markerstrokewidth=0.1, color=1, label=[\"Noisy Data\" \"\"])","category":"page"},{"location":"gettingstarted/#Computing-the-negative-log-likelihood","page":"Fenrr.jl in a nutshell","title":"Computing the negative log-likelihood","text":"","category":"section"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"To do parameter inference - be it maximum-likelihod, maximum a posteriori, or full Bayesian inference with MCMC - we need to evaluate the likelihood of given a parameter estimate theta_textest. This is exactly what Fenrir.jl's fenrir_nll provides:","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"p_est = (0.1, 0.1, 2.0)\nprob = remake(true_prob, p=p_est)\ndata = (t=times, u=odedata)\nκ² = 1e10\nnll, _, _ = fenrir_nll(prob, data, observation_noise_var, κ²; dt=1e-1)\nnll","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"This is the negative marginal log-likelihood of the parameter p_est. You can use it as any other NLL: Optimize it to compute maximum-likelihood estimates or MAPs, or plug it into MCMC to sample from the posterior. In our paper we compute MLEs by pairing Fenrir with Optimization.jl and ForwardDiff.jl. Let's quickly explore how to do this.","category":"page"},{"location":"gettingstarted/#Maximum-likelihood-parameter-inference","page":"Fenrr.jl in a nutshell","title":"Maximum-likelihood parameter inference","text":"","category":"section"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"To compute a maximum-likelihood estimate (MLE), we just need to maximize theta to p(mathcalD mid theta) - that is, minimize the nll from above. We use Optimization.jl for this. First, fefine a loss function and create an OptimizationProblem","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"function loss(x, _)\n    ode_params = x[begin:end-1]\n    prob = remake(true_prob, p=ode_params)\n    κ² = exp(x[end]) # the diffusion parameter of the EK1\n    return fenrir_nll(prob, data, observation_noise_var, κ²; dt=1e-1)\nend\n\nfun = OptimizationFunction(loss, Optimization.AutoForwardDiff())\noptprob = OptimizationProblem(\n    fun, [p_est..., 1e0];\n    lb=[0.0, 0.0, 0.0, -10], ub=[1.0, 1.0, 5.0, 20]\n)","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"Then, just solve it! Here we use LBFGS:","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"optsol = solve(optprob, LBFGS())\np_mle = optsol.u[1:3]\np_mle # hide","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"That's it! The computed MLE is very close to the true parameter which we used to generate the data. As a final step, let's plot the true solution, the data, and the result of the MLE:","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"plot(true_sol, color=:black, linestyle=:dash, label=[\"True Solution\" \"\"])\nscatter!(times, stack(odedata), markersize=2, markerstrokewidth=0.1, color=1, label=[\"Noisy Data\" \"\"])\nmle_sol = solve(remake(true_prob, p=p_mle), EK1())\nplot!(mle_sol, color=3, label=[\"MLE-parameter Solution\" \"\"])","category":"page"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"Looks good!","category":"page"},{"location":"gettingstarted/#References","page":"Fenrr.jl in a nutshell","title":"References","text":"","category":"section"},{"location":"gettingstarted/","page":"Fenrr.jl in a nutshell","title":"Fenrr.jl in a nutshell","text":"[1] F.Tronarp, N. Bosch, P. Hennig: Fenrir: Fenrir: Physics-Enhanced Regression for Initial Value Problems (2022)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Fenrir","category":"page"},{"location":"#Fenrir","page":"Home","title":"Fenrir","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Fenrir exports just a single function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"fenrir_nll","category":"page"},{"location":"#Fenrir.fenrir_nll","page":"Home","title":"Fenrir.fenrir_nll","text":"fenrir_nll(prob::ODEProblem, data::NamedTuple{(:t, :u)}, observation_noise_var::Real,\n    diffusion_var::Union{Real,Vector{<:Real}};\n    adaptive=false, dt=false,  proj=I, order=3::Int, tstops=[])\n\nCompute the \"Fenrir\" approximate negative log-likelihood (NLL) of the data.\n\nThis is a convenience function that\n\nSolves the ODE with a ProbNumDiffEq.EK1 of the specified order and with a diffusion as provided by the diffusion_var argument, and\nFits the ODE posterior to the data via Kalman filtering and thereby computes the negative log-likelihood on the way.\n\nBy default, the solver steps exactly through the time points data.t. In addition, you can provide a step size with dt or time stops with tstops. Or, set adaptive=true for adaptive step-size selection - use at your own risk!\n\nReturns a tuple (nll::Real, times::Vector{Real}, states::StructVector{Gaussian}), where states contains the filtering posterior. Its mean and covariance can be accessed with states.μ and states.Σ.\n\nArguments\n\nprob::ODEProblem: the initial value problem of interest\ndata::NamedTuple{{(:t, :u)}}: the data to be fitted\nobservation_noise_var::Real: the scalar observation noise variance\ndiffusion_var: the diffusion parameter for the integrated Wiener process prior; this plays a similar role as kernel hyperparamers in Gaussian-process regression\ndt=false: step size parameter, passed to OrdinaryDiffEq.init\nadaptive=false: flag to determine if adaptive step size selection should be used; use at your own risk!\ntstops=[]: additional time stops the algorithm should step through; passed to OrdinaryDiffEq.solve\norder::Int=3: the order of the ProbNumDiffEq.EK1 solver\nproj=I: the matrix which maps the ODE state to the measurements; typically a projection\n\n\n\n\n\n","category":"function"}]
}
